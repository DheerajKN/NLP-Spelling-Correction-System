{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;margin:-7px\">Intro to Natural Language Processing</h1>\n",
    "<h2 style=\"text-align: center;\">Assignment 1</h2>\n",
    "<h3 style=\"text-align: center;margin:7px\">Student ID: 20231272</h3>\n",
    "<h3 style=\"text-align: center;margin:3px\">Student Name: Kollapudi Nagendra Dheeraj</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2oP1uun77cIh"
   },
   "source": [
    "# Assignment 1\n",
    "\n",
    "This assignment will involve the creation of a spellchecking system and an evaluation of its performance. You may use the code snippets provided in Python for completing this or you may use the programming language or environment of your choice\n",
    "\n",
    "Please start by downloading the corpus `holbrook.txt` from Blackboard\n",
    "\n",
    "The file consists of lines of text, with one sentence per line. Errors in the line are marked with a `|` as follows\n",
    "\n",
    "    My siter|sister go|goes to Tonbury .\n",
    "    \n",
    "In this case the word 'siter' was corrected to 'sister' and the word 'go' was corrected to 'goes'.\n",
    "\n",
    "In some places in the corpus two words maybe corrected to a single word or one word to a multiple words. This is denoted in the data using underscores e.g.,\n",
    "\n",
    "    My Mum goes out some_times|sometimes .\n",
    "    \n",
    "For the purpose of this assignment you do not need to separate these words, but instead you may treat them like a single token.\n",
    "\n",
    "*Note: you may use any functions from NLTK to complete the assignment. It should not be necessary to use other libraries and so please consult with us if your solution involves any other external library. If you use any function from NLTK in Task 6 please include a brief description of this function and how it contributes to your solution.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TIVCSJV-7kDs"
   },
   "source": [
    "## Task 1 (10 Marks)\n",
    "\n",
    "Write a parser that can read all the lines of the file `holbrook.txt` and print out for each line the original (misspelled) text, the corrected text and the indexes of any changes. The indexes refers to the index of the words in the sentence. In the example given, there is only an error in the 10th word and so the list of indexes is [9]. It is not necessary to analyze where the error occurs inside the word.\n",
    "\n",
    "Then split your data into a test set of 100 lines and a training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk\n",
    "import nltk\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import words\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.collocations import *\n",
    "\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "RznCZ1mw7mfk"
   },
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "def parser(txt_file_name):\n",
    "    \n",
    "    #Reading the txt file and extracting all the lines\n",
    "    lines = open(txt_file_name).readlines()\n",
    "    data = []\n",
    "    \n",
    "    #Based on the assert function below, we have to break the list of sentences into list of words via tokenizing the sentences\n",
    "    worded_list = [word_tokenize(sent) for sent in lines]\n",
    "    \n",
    "    #Now we have to search for | and create assert style, dictionary structure with a left | value in original and right | value\n",
    "    #in corrected along with keeping track of the index where | is present.\n",
    "    for word_list in worded_list:\n",
    "        #First search inside the list for |\n",
    "        ##As | is not a individual value like ['friend', 'is', 'sic', '|', 'sick'] inside a list \n",
    "        ###but is embedded in a element of list ['friend', 'is', 'sic|sick'] so direct in wont work, but double for is required.\n",
    "        \n",
    "        #Creating these varibables for each list element present inside main list, as below assert requirement is in such fashion.\n",
    "        original_string = []\n",
    "        corrected_string = []\n",
    "        correction_index = []\n",
    "        count_index = 0\n",
    "        for word in word_list:\n",
    "            if '|' in word:\n",
    "                \n",
    "                #Then split the word for |\n",
    "                splitted_context = word.split('|')\n",
    "                \n",
    "                # Left aka wrong word is stored to original.\n",
    "                original_string.append(splitted_context[0])\n",
    "                \n",
    "                # Right aka correct word is stored to corrected.\n",
    "                corrected_string.append(splitted_context[1])\n",
    "                \n",
    "                #Noting down the correction index.\n",
    "                correction_index.append(count_index)\n",
    "        \n",
    "            else:\n",
    "                # If there is no '|' in word present inside the list, then store that word original and corrected listed strings.\n",
    "                original_string.append(word)\n",
    "                corrected_string.append(word)\n",
    "\n",
    "            #Keep track of index until if correction index is found\n",
    "            count_index += 1\n",
    "\n",
    "        #Loading to original and corrected listed strings along with the correction_index to dictionary.      \n",
    "        dictionary = {'original': original_string, 'corrected': corrected_string, 'indexes': correction_index}\n",
    "        data.append(dictionary)\n",
    "    return data\n",
    "    \n",
    "data = parser(\"holbrook.txt\") \n",
    "\n",
    "assert(data[2] == {\n",
    "   'original': ['I', 'have', 'four', 'in', 'my', 'Family', 'Dad', 'Mum', 'and', 'siter', '.'], \n",
    "   'corrected': ['I', 'have', 'four', 'in', 'my', 'Family', 'Dad', 'Mum', 'and', 'sister', '.'], \n",
    "   'indexes': [9]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eRSX4I0H7pSC"
   },
   "source": [
    "The counts and assertions given in the following sections are based on splitting the training and test set as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Kt9aR2Gy7p1C"
   },
   "outputs": [],
   "source": [
    "test = data[:100]\n",
    "train = data[100:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hm5JL7cH7sLK"
   },
   "source": [
    "## **Task 2** (10 Marks): \n",
    "Calculate the frequency (number of occurrences), *ignoring case*, of all words and their unigram probability from the corrected *training* sentences.\n",
    "\n",
    "*Hint: use `Counter` to implement this so it may be called many times*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ReStructuring the Dictionary of elements into list of words so that it can be used for processing\n",
    "train_corrected = [dictionary_element['corrected'] for dictionary_element in train]\n",
    "\n",
    "#Combing the list into one huge string so that the word_tokenize function could process it.\n",
    "train_corrected = [\" \".join(dictionary_element) for dictionary_element in train_corrected]\n",
    "train_set_fit_for_processing = \" \".join(train_corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "7ge0uHS-7uEK"
   },
   "outputs": [],
   "source": [
    "#Tokenize the sentences into words\n",
    "tokenized_train_set_fit_for_processing = word_tokenize(train_set_fit_for_processing)\n",
    "length_of_training_set = len(tokenized_train_set_fit_for_processing)\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "#Applied Counter to get values of occurrences of each element in the list\n",
    "fdist = Counter(tokenized_train_set_fit_for_processing)\n",
    "\n",
    "#Extracting unique words using keys() so that it can be used inside preprocess_check and get_candidates\n",
    "unique_words = list(fdist.keys())\n",
    "\n",
    "def unigram(word):\n",
    "    return fdist[word]\n",
    "    \n",
    "\n",
    "def prob(word):\n",
    "    return unigram(word)/length_of_training_set\n",
    "\n",
    "# Test your code with the following\n",
    "assert(unigram(\"me\")==87)\n",
    "assert(round(prob(\"me\"),3)==0.004)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w8r8QYj78GPK"
   },
   "source": [
    "## **Task 3** (15 Marks): \n",
    "[Edit distance](https://en.wikipedia.org/wiki/Edit_distance) is a method that calculates how similar two strings are to one another by counting the minimum number of operations required to transform one string into the other. There is a built-in implementation in NLTK that works as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 956,
     "status": "ok",
     "timestamp": 1536070558621,
     "user": {
      "displayName": "John McCrae",
      "photoUrl": "//lh3.googleusercontent.com/-whXIBV_wL0Y/AAAAAAAAAAI/AAAAAAAAATE/-2hfaPZsyHM/s50-c-k-no/photo.jpg",
      "userId": "102173405218988557336"
     },
     "user_tz": -60
    },
    "id": "SV9Mu8P38IQE",
    "outputId": "9f29e22b-0f8b-4b92-9d5f-fcde3efec970"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "from nltk.metrics.distance import edit_distance\n",
    "\n",
    "# Edit distance returns the number of changes to transform one word to another\n",
    "print(edit_distance(\"hello\", \"hi\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hm46Lbiz8K8M"
   },
   "source": [
    "Write a function that calculates all words with *minimal* edit distance to the misspelled word. You should do this as follows\n",
    "\n",
    "1. Collect the set of all unique tokens in `train`\n",
    "2. Find the minimal edit distance, that is the lowest value for the function `edit_distance` between `token` and a word in `train`\n",
    "3. Output all unique words in `train` that have this same (minimal) `edit_distance` value\n",
    "\n",
    "*Do not implement edit distance, use the built-in NLTK function `edit_distance`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "HoilAmFW8PCb"
   },
   "outputs": [],
   "source": [
    "def get_candidates(token):\n",
    "    #Creating a dictionary of words as key and their edit_distance as value for each unique_word\n",
    "    word_distances = dict((word, edit_distance(word, token)) for word in unique_words)\n",
    "    \n",
    "    #Sort the dictionary based on values from min to max and get the first element\n",
    "    minimal_dist = sorted(word_distances.values())[0]\n",
    "    \n",
    "    #Filter out and get those words that have a minimal distance\n",
    "    return list(filter(lambda k: word_distances[k] == minimal_dist, word_distances.keys()))\n",
    "        \n",
    "# Test your code as follows\n",
    "assert get_candidates(\"minde\") == ['mine', 'mind']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RGY-eCkN8TIM"
   },
   "source": [
    "## Task 4 (15 Marks):\n",
    "\n",
    "Write a function that takes a (misspelled) sentence and returns the corrected version of that sentence. The system should scan the sentence for words that are not in the dictionary (set of unique words in the training set) and for each word that is not in the dictionary choose a word in the dictionary that has minimal edit distance and has the highest *unigram probability*. \n",
    "\n",
    "*Your solution to this should involve `get_candidates`*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "dIGKE4_P8WGP"
   },
   "outputs": [],
   "source": [
    "def correct(sentence):\n",
    "    corrected = []\n",
    "   \n",
    "    for i in sentence:\n",
    "        # If word not in unique word the calculate suggested words with minimal distance\n",
    "        if i.lower() not in unique_words:\n",
    "            \n",
    "            #Fetch all the candidates that match the candidate\n",
    "            candidates = get_candidates(i)\n",
    "            \n",
    "            #If there are more than 1 values that have same similarity\n",
    "            if len(candidates) > 1:\n",
    "                probabilities = {}\n",
    "                \n",
    "                #Retrieve their probabilites as per the trained model\n",
    "                for candidate in candidates:\n",
    "                    probabilities[candidate] = prob(candidate)\n",
    "                \n",
    "                #Sort Dictionary values but in reverse as we want the highest and return top element\n",
    "                highest_prob = sorted(probabilities.values(), reverse=True)[0]\n",
    "                \n",
    "                #Filter Dictionary whose values match the highest prob one and return only the keys\n",
    "                candidates_with_highest_prob = list(\n",
    "                    filter(lambda k: probabilities[k] == highest_prob, probabilities.keys()))\n",
    "                \n",
    "                #Return the candidate with the highest probabilities\n",
    "                corrected.append(candidates_with_highest_prob[0])\n",
    "            else:\n",
    "                corrected.append(get_candidates(i)[0])\n",
    "\n",
    "        else:\n",
    "            corrected.append(i)\n",
    "    return corrected\n",
    "\n",
    "assert(correct([\"this\",\"whitr\",\"cat\"]) == ['this','white','cat'])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oG7jC6au8kka"
   },
   "source": [
    "## **Task 5** (10 Marks): \n",
    "Using the test corpus evaluate the *accuracy* of your method, i.e., how many words from your system's output match the corrected sentence (you should count words that are already spelled correctly and not changed by the system)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 539,
     "status": "ok",
     "timestamp": 1536071822989,
     "user": {
      "displayName": "John McCrae",
      "photoUrl": "//lh3.googleusercontent.com/-whXIBV_wL0Y/AAAAAAAAAAI/AAAAAAAAATE/-2hfaPZsyHM/s50-c-k-no/photo.jpg",
      "userId": "102173405218988557336"
     },
     "user_tz": -60
    },
    "id": "HSXTQypR8mdR",
    "outputId": "853813e4-d71b-42a7-8e96-68d038457628"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the system:  81.9971390685487\n"
     ]
    }
   ],
   "source": [
    "def accuracy(test):\n",
    "    accuracy = 0\n",
    "    \n",
    "    #Iterate through each of the statement\n",
    "    for _, individual_test_dictionary in enumerate(test):\n",
    "        corrected_string = individual_test_dictionary['corrected']\n",
    "        predicted_string = correct(individual_test_dictionary['original'])\n",
    "        \n",
    "        #Calculate the accuracy by getting the length of words from system's output and corrected sentence\n",
    "        #by taking set intersection divided by the length of the corrected string\n",
    "        accuracy += len([i for i, j in zip(corrected_string, predicted_string) if i == j])/len(corrected_string)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "print(\"Accuracy of the system: \", accuracy(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9b-r2JzD8_Zh"
   },
   "source": [
    "## **Task 6 (35 Marks):**\n",
    "\n",
    "Consider a modification to your algorithm that would improve the accuracy of the algorithm developed in Task 3 and 4\n",
    "\n",
    "* You may resources beyond those provided here.\n",
    "* You must **not use the test data** in this task.\n",
    "* Provide a short text describing what you intend to do and why. \n",
    "* Full marks for this section may be obtained without an implementation, but an implementation is preferred.\n",
    "* Your implementation should not consist of more than 50 lines of code\n",
    "\n",
    "Please note this task is marked according to: demonstration of knowledge from the lectures (10), originality and appropriateness of solution (10), completeness of description (10) and technical correctness (5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enhancements done to the algorithm:\n",
    "\n",
    "* Inorder to enhance the algorithm we need to enhance the accuracy. Based on checking the predicted and corrected for each sentence by looking over training data. I was able to derive 2 ways that help compensate this drop in accuracy. \n",
    "    \n",
    "    \n",
    "    One way is by reducing accuracy drop which is because of:     \n",
    "        * Unwanted processing.\n",
    "            * Correct words, punctation or digits were getting processed\n",
    "            * Words that are Proper Nouns are getting processed.\n",
    "         * Solution - Some kind of pre-process checking so that these unnecessary words won't get processed\n",
    "         \n",
    "         \n",
    "    Another way is by increasing accuracy by: \n",
    "        * Finding appropriate word based on surrounding words.\n",
    "            * Finding the likelihood of the current word based on the previous word.\n",
    "         * Solution - Using nltk package to detect the exact word and send it as a additional list for the system to detect the relevancy via the probabilities from get_candidates() function\n",
    "       \n",
    "   * Processing these pain points, few using python packages and rest using NLTK helped rectify the issues with previous correct function that helping in increasing accuracy by a **good** margin.\n",
    "   \n",
    "## Unwanted processing:\n",
    "### Implemented under -> preprocess_check(word):\n",
    "**Why?:** On viewing few incorrect predictions like [a, at] [guts, nuts] [their, there] etc. It is pretty much clear that some level of unwanted processing is happening as the words on the left are correct and there is no spelling correction needed. It is processing them to values on the right. So in order to stop this a level of pre-process check was implemented to make sure a, guts, their don't enter into get_candidates() function.\n",
    "\n",
    "#### 1. Unique Words:\n",
    " As the algorithm is structured in such a way that it processes any words it sees, so some of the words need not to get processed. So using the unique words trained from the FreqDist from unigram processing, I extracted the keys and added it into preprocess criterion so that these words that match with list do not get processed. Along with the unique_word inclusion of WordNet and isDigit were also added to reduce the processing time by a significant amount.\n",
    "\n",
    "#### 2. WordNet:\n",
    " As Unique words are trained over corpus it might not be sufficient for all the cases, so we need a library in determining whether a word is proper or not. So I used wordnet.words() from WordNet Library, which has a range of values that can help in identifing whether if a word in the sentence is correct, and if so don't consider it for processing.\n",
    "\n",
    "#### 3. IsDigit:\n",
    " Along with the processing of some unnecessary words, I also considered not to process number and special characters as there processing is not needed in the first place, and as they are not part of any wordnet dictionaries. So checking for the presence of numbers, if present then not include it in the processing.\n",
    " \n",
    "#### 4. string.punctation:\n",
    " As mentioned above with the interest of time, unnecessary processing these is of no use, so neglecting them as part of special condition as punctation or special characters are not part of wordnet dictionaries. So checking for the presence of special characters, if present then not include it in the processing.\n",
    " \n",
    "#### 5. Proper Nouns:\n",
    " Some proper names in the corpus such as \"End\", \"Honda\", \"JOAN\" ,\"STALL\" is getting processed even though they are right or wrong it shouldn't. So using the nltk.tag.pos_tag to check if the word's part of speech is not starting with 'NN'. This addition proved helpful as the algorithm was able to receive better boost in accuracy. So Implemented the same.\n",
    "\n",
    "## Finding appropriate word based on surrounding words:\n",
    "### Implemented under -> likelihood_words(word)\n",
    "#### 6. BigramCollocation:\n",
    " BigramCollocation was used here to find adquate word for the mispelled word considering the likelihood of one word on the left or right from the mispelled word based on the train corpus. So basically I'm sending this extra result to the get_candidates function so that it can help predict a better fit for the mispelled word, but it didn't give any significant boost as the test cases might have had most of the scenarios not seen during BigramCollocationFinder phase. An assert statement was added for better info.\n",
    "\n",
    "## Some more ways the accuracy could be increased: (Not Implemented)\n",
    "#### 7. Presence of more Training Data:\n",
    " Presence of more data to train for would for sure enhance the accuracy levels. Atleast some overlap between the train and test scenarios would help the BigramCollocation could work better.\n",
    "\n",
    "#### 8. Presence of some vectorized word denotation for context instead of simple word list:\n",
    " As mentioned above, nltk's words.words() contains few confusing words which result incorrect word estimation. Hence, instead of words.words() if presence of some vectorized format to fetch the words based on neighbouring words and considering cosine similarity might result in slight increase in accuracy.\n",
    " \n",
    "#### 10. Consideration of Bigram and Trigram Probabilities:\n",
    " Bigram, Trigram and n-gram probabilities could also be introduced to check the most probabilistic word to the mispelled considering the left n words or right n words and passing them to their probabilistic function to check the word with highest index and retrieving it rather than simple unigram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_check(word):\n",
    "    conditions = []\n",
    "    \n",
    "    #Checking if the word is not present in the train_corrected unique set list\n",
    "    conditions.append(word.lower() not in unique_words)\n",
    "    \n",
    "    #Check if word in itself is valid or not\n",
    "    conditions.append(word not in wn.words())\n",
    "\n",
    "    #Check if word is not a digit\n",
    "    conditions.append(not word.isdigit())\n",
    "    \n",
    "    #Check if word is a special character\n",
    "    conditions.append(word not in string.punctuation)\n",
    "    \n",
    "    #Check if word is not a noun or proper noun using pos_tag.\n",
    "    pos_status = pos_tag([word])[0][1]\n",
    "    conditions.append(not pos_status.startswith('NN'))\n",
    "    \n",
    "    return all(conditions)\n",
    "\n",
    "assert(preprocess_check('JOAN') == False)\n",
    "\n",
    "Bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(word_tokenize(train_set_fit_for_processing))\n",
    "\n",
    "def likelihood_words(word):\n",
    "    return [j for (i, j), v in finder.score_ngrams(Bigram_measures.raw_freq) if i == word]\n",
    "\n",
    "assert(likelihood_words('Japanese') == ['Honda'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhanced_correct(sentence):\n",
    "    corrected = []\n",
    "    \n",
    "    #Iterate through each word present in the statement\n",
    "    for i, word in enumerate(sentence):\n",
    "        \n",
    "        #Check if word doesn't satisfies the above conditions, if so go ahead with pre-processing.\n",
    "        ##otherwise directly append to final output.\n",
    "        if preprocess_check(word):\n",
    "\n",
    "            #Get all the related candidates from unigram and bigramcollation processing, if the word is wrong\n",
    "            candidates = get_candidates(word) + likelihood_words(sentence[i-1])\n",
    "            \n",
    "            #If there are more candidates\n",
    "            if len(candidates) > 1:\n",
    "                probabilities = {}\n",
    "                \n",
    "                #Retrieve their probabilites as per the trained model\n",
    "                for candidate in candidates:\n",
    "                    probabilities[candidate] = prob(candidate)\n",
    "                    \n",
    "                #Sort Dictionary values but in reverse as we want the highest and return top element\n",
    "                highest_prob = sorted(probabilities.values(), reverse=True)[0]\n",
    "                \n",
    "                #Filter Dictionary whose values match the highest prob one and return only the keys\n",
    "                candidates_with_highest_prob = list(\n",
    "                    filter(lambda k: probabilities[k] == highest_prob, probabilities.keys()))\n",
    "                \n",
    "                #Return the candidate with the highest probabilities\n",
    "                corrected.append(candidates_with_highest_prob[0])\n",
    "            \n",
    "            #Else give the first element\n",
    "            else:\n",
    "                corrected.append(candidates[0])\n",
    "\n",
    "        else:\n",
    "            corrected.append(word)\n",
    "\n",
    "    return corrected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GLzaC6D28sK9"
   },
   "source": [
    "## **Task 7 (5 Marks):**\n",
    "\n",
    "Repeat the evaluation (as in Task 5) of your new algorithm and show that it outperforms the algorithm from Task 3 and 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Hw6PzwWn7iEo",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the system:  88.70299035135751 %\n"
     ]
    }
   ],
   "source": [
    "def accuracy_mod(test):\n",
    "    accuracy = 0\n",
    "    \n",
    "    #Iterate through each of the statement\n",
    "    for _, individual_test_dictionary in enumerate(test):\n",
    "        corrected_string = individual_test_dictionary['corrected']\n",
    "        predicted_string = enhanced_correct(individual_test_dictionary['original'])\n",
    "        \n",
    "        #Calculate the accuracy by getting the length of words from system's output and corrected sentence\n",
    "        #by taking set intersection divided by the length of the corrected string\n",
    "        accuracy += len([i for i, j in zip(corrected_string, predicted_string) if i == j])/len(corrected_string)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "print(\"Accuracy of the system: \", accuracy_mod(test), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enhanced Model Evaluation:\n",
    "\n",
    "By Implementing all the above mentioned methods of reducing processing and finding alternatives, an accuracy boost from 82% to 88% was seen and resulting in a 6% increase from Task 5 to Task 7.\n",
    "\n",
    "### Corresponding Results\n",
    "Accuracy of the modified system:  88.7 %"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "CT5120/CT5146 - Assignment 1",
   "provenance": [
    {
     "file_id": "12crGPce24mcgITZPs7hU_9CPnLAcyIq6",
     "timestamp": 1603097790764
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
